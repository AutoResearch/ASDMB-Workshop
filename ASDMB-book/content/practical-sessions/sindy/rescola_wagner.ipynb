{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Tutorial: Recovering Rescorla-Wagner Parameters from a Bandit Task using SINDy\n",
    ">[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/AutoResearch/ASDMB-Workshop/blob/main/ASDMB-book/content/practical-sessions/sindy/rescorla_wagner.ipynb)\n",
    "\n",
    "This notebook demonstrates a more advanced application of SINDy for model discovery in cognitive science. We will simulate an agent learning in a classic psychological task (a two-armed bandit) and then use SINDy to recover the agent's hidden learning parameters from its behavior.\n",
    "\n",
    "### The Goal\n",
    "\n",
    "Our goal is to simulate behavioral data from a known cognitive model and then play the role of a scientist. Given only the agent's 'observable' data (its internal value estimates over time), can we use SINDy to discover the mathematical rule it uses for learning and, from that rule, deduce its specific learning parameters?\n",
    "\n",
    "### The Workflow\n",
    "1.  **Define a Model & Environment**: We'll create a `RescorlaWagnerAgent` and a `TwoArmedBandit` environment.\n",
    "2.  **Generate Data**: We'll simulate agents performing the task to create a dataset.\n",
    "3.  **Apply SINDy**: We'll use the `pysindy` library to analyze an agent's learning trajectory.\n",
    "4.  **Recover Parameters**: We'll interpret the SINDy model to extract the original learning rate (`α`) and reward magnitude (`λ`).\n",
    "5.  **Visualize & Compare**: We'll compare SINDy's performance to a standard linear regression approach and visualize the results.\n"
   ],
   "id": "f10200ec7b69a48a"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 1: Setup and Imports\n",
    "\n",
    "First, we'll install the necessary packages if they are not already present and then import them.\n"
   ],
   "id": "abb7c805904dec4c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Run this cell if you don't have the packages installed\n",
    "!pip install pysindy scikit-learn pandas matplotlib scipy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import pysindy as ps\n",
    "from scipy.integrate import solve_ivp\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ],
   "id": "38fa4d544664a68b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class RescorlaWagnerAgent:\n",
    "    \"\"\"Agent implementing Rescorla-Wagner learning dynamics.\"\"\"\n",
    "\n",
    "    def __init__(self, learning_rate=0.1, initial_value=0.0):\n",
    "        self.learning_rate = learning_rate  # α (alpha)\n",
    "        self.value = initial_value\n",
    "        self.value_history = [initial_value]\n",
    "\n",
    "    #tutorial assignment:\n",
    "    #Update value according to the discrete Rescorla-Wagner rule.\n",
    "    def update_value(self, reward, reward_magnitude=1.0):\n",
    "        #Your code here\n",
    "\n",
    "\n",
    "\n",
    "    def reset(self, initial_value=0.0):\n",
    "        \"\"\"Reset agent to initial state.\"\"\"\n",
    "    #Your code here\n",
    "\n",
    "\n",
    "\n"
   ],
   "id": "28cd8b76bf0014bb"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### The Two-Armed Bandit Environment\n",
    "\n",
    "This is a simple task where an agent must choose between two slot machines ('arms'), each with a different probability of giving a reward.\n"
   ],
   "id": "6efc6db85f23f91a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "class TwoArmedBandit:\n",
    "    \"\"\"Two-armed bandit environment with different reward probabilities.\"\"\"\n",
    "\n",
    "    def __init__(self, arm1_prob=0.7, arm2_prob=0.3, reward_magnitude=1.0):\n",
    "        self.arm1_prob = arm1_prob\n",
    "        self.arm2_prob = arm2_prob\n",
    "        self.reward_magnitude = reward_magnitude\n",
    "\n",
    "    def pull_arm(self, arm):\n",
    "        \"\"\"Pull specified arm and return reward.\"\"\"\n",
    "        if arm == 0:\n",
    "            prob = self.arm1_prob\n",
    "        else:\n",
    "            prob = self.arm2_prob\n",
    "\n",
    "        return self.reward_magnitude if np.random.random() < prob else 0.0\n"
   ],
   "id": "7fce3e39e196b999"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Step 3: Generating the Behavioral Dataset\n",
    "\n",
    "Now we'll simulate agents playing the bandit task. This function will be our data generator.\n"
   ],
   "id": "44a216f92906a2b0"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def generate_bandit_dataset(n_trials=100, n_agents=50, learning_rate=0.1,\n",
    "                           arm1_prob=0.7, arm2_prob=0.3, reward_magnitude=1.0,\n",
    "                           exploration_rate=0.1):\n",
    "    \"\"\"Generate bandit task dataset with Rescorla-Wagner learning.\"\"\"\n",
    "    bandit = TwoArmedBandit(arm1_prob, arm2_prob, reward_magnitude)\n",
    "    data = []\n",
    "\n",
    "    for agent_id in range(n_agents):\n",
    "        # Each agent is a new instance with its own history\n",
    "        agent = RescorlaWagnerAgent(learning_rate=learning_rate)\n",
    "\n",
    "        for trial in range(n_trials):\n",
    "            # Choose action (ε-greedy with exploration)\n",
    "            if np.random.random() < exploration_rate:\n",
    "                action = np.random.choice([0, 1]) # Explore\n",
    "            else:\n",
    "                action = 0 if agent.value > 0.5 else 1 # Exploit (simplified policy)\n",
    "\n",
    "            # Get reward from the environment\n",
    "            reward = bandit.pull_arm(action)\n",
    "\n",
    "            # Agent updates its internal value\n",
    "            # Note: reward_magnitude is the max possible reward, not the actual reward received\n",
    "            agent.update_value(reward, reward_magnitude=reward_magnitude)\n",
    "\n",
    "            data.append({\n",
    "                'agent_id': agent_id,\n",
    "                'trial': trial,\n",
    "                'action': action,\n",
    "                'reward': reward,\n",
    "                'value': agent.value,\n",
    "                'learning_rate': learning_rate,\n",
    "                'reward_magnitude': reward_magnitude\n",
    "            })\n",
    "\n",
    "    return pd.DataFrame(data)\n"
   ],
   "id": "2125946eed8a1425"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Set our experimental parameters\n",
    "n_trials = 200\n",
    "n_agents = 50\n",
    "true_alpha = 0.15  # This is the learning rate we want to recover\n",
    "true_lambda = 0.7    # This is the reward magnitude we want to recover\n",
    "\n",
    "# Generate the dataset\n",
    "data = generate_bandit_dataset(\n",
    "    n_trials=n_trials,\n",
    "    n_agents=n_agents,\n",
    "    learning_rate=true_alpha,\n",
    "    reward_magnitude=true_lambda\n",
    ")\n",
    "\n",
    "print(f\"Dataset generated with {len(data)} total entries.\")\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "data.head()\n"
   ],
   "id": "3b9f4c6cd9a624c1"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "def apply_sindy_to_recover_parameters(data, agent_id=0):\n",
    "    \"\"\"Apply SINDy to recover Rescorla-Wagner parameters from agent data.\"\"\"\n",
    "    agent_data = data[data['agent_id'] == agent_id].sort_values('trial')\n",
    "\n",
    "    # Prepare data for SINDy: time and the state variable (V)\n",
    "     #Your code here\n",
    "\n",
    "    # 1. Define how to calculate the derivative from data\n",
    "     #Your code here\n",
    "\n",
    "    # 2. Define the library of possible functions (we expect a constant and a linear term)\n",
    "     #Your code here\n",
    "\n",
    "    # 3. Define the sparse optimizer\n",
    "     #Your code here\n",
    "\n",
    "    # 4. Build and fit the SINDy model\n",
    "     #Your code here\n",
    "\n",
    "\n",
    "    # 5. Extract coefficients and solve for RW parameters\n",
    "     #Your code here\n",
    "\n",
    "    return {\n",
    "        'model': model,\n",
    "        'coefficients': coefficients,\n",
    "        'recovered_alpha': recovered_alpha,\n",
    "        'recovered_lambda': recovered_lambda,\n",
    "        'time': time,\n",
    "        'value': value\n",
    "    }\n"
   ],
   "id": "47db38770acfadc7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Apply SINDy to recover parameters\n",
    "agent_id_to_analyze = 0\n",
    "sindy_results = apply_sindy_to_recover_parameters(data, agent_id_to_analyze)\n",
    "\n",
    "print(\"SINDy Parameter Recovery Results:\")\n",
    "print(f\"Recovered α (learning rate): {sindy_results['recovered_alpha']:.4f}\")\n",
    "print(f\"Recovered λ (reward magnitude): {sindy_results['recovered_lambda']:.4f}\")\n",
    "print(f\"True α: {true_alpha:.4f}\")\n",
    "print(f\"True λ: {true_lambda:.4f}\")\n",
    "print(f\"Error in α: {abs(sindy_results['recovered_alpha'] - true_alpha):.4f}\")\n",
    "print(f\"Error in λ: {abs(sindy_results['recovered_lambda'] - true_lambda):.4f}\")\n",
    "\n",
    "# Print the discovered equation\n",
    "print(f\"\\nSINDy discovered equation:\")\n",
    "print(f\"dV/dt = {sindy_results['recovered_alpha']:.4f} * ({sindy_results['recovered_lambda']:.4f} - V)\")\n"
   ],
   "id": "811221f0f11bc98f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Visualize the results\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "# Plot 1: Learning trajectory\n",
    "plt.subplot(1, 3, 1)\n",
    "plt.plot(sindy_results['time'], sindy_results['value'], 'b-', linewidth=2, label='Observed Value')\n",
    "plt.axhline(y=true_lambda, color='r', linestyle='--', label=f'True λ = {true_lambda}')\n",
    "plt.xlabel('Trial')\n",
    "plt.ylabel('Value (V)')\n",
    "plt.title('Learning Trajectory')\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Parameter comparison\n",
    "plt.subplot(1, 3, 2)\n",
    "methods = ['True', 'SINDy']\n",
    "alpha_values = [true_alpha, sindy_results['recovered_alpha']]\n",
    "lambda_values = [true_lambda, sindy_results['recovered_lambda']]\n",
    "\n",
    "x = np.arange(len(methods))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, alpha_values, width, label='α (learning rate)', alpha=0.8)\n",
    "plt.bar(x + width/2, lambda_values, width, label='λ (reward magnitude)', alpha=0.8)\n",
    "plt.xlabel('Method')\n",
    "plt.ylabel('Parameter Value')\n",
    "plt.title('Parameter Recovery')\n",
    "plt.xticks(x, methods)\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: SINDy model equation\n",
    "plt.subplot(1, 3, 3)\n",
    "equation_text = f\"SINDy Discovered Equation:\\n\\ndV/dt = {sindy_results['recovered_alpha']:.4f} * ({sindy_results['recovered_lambda']:.4f} - V)\\n\\nTrue Equation:\\ndV/dt = {true_alpha:.4f} * ({true_lambda:.4f} - V)\"\n",
    "plt.text(0.1, 0.5, equation_text, fontsize=12, verticalalignment='center',\n",
    "         bbox=dict(boxstyle=\"round,pad=0.3\", facecolor=\"lightblue\"))\n",
    "plt.xlim(0, 1)\n",
    "plt.ylim(0, 1)\n",
    "plt.title('Model Comparison')\n",
    "plt.axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ],
   "id": "66a054549823d09"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
